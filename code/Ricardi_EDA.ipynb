{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>via</th>\n",
       "      <th>description</th>\n",
       "      <th>extensions</th>\n",
       "      <th>job_id</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>...</th>\n",
       "      <th>commute_time</th>\n",
       "      <th>salary_pay</th>\n",
       "      <th>salary_rate</th>\n",
       "      <th>salary_avg</th>\n",
       "      <th>salary_min</th>\n",
       "      <th>salary_max</th>\n",
       "      <th>salary_hourly</th>\n",
       "      <th>salary_yearly</th>\n",
       "      <th>salary_standardized</th>\n",
       "      <th>description_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Meta</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>In the intersection of compliance and analytic...</td>\n",
       "      <td>['15 hours ago', '101K–143K a year', 'Work fro...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101K–143K</td>\n",
       "      <td>a year</td>\n",
       "      <td>122000.0</td>\n",
       "      <td>101000.0</td>\n",
       "      <td>143000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122000.0</td>\n",
       "      <td>122000.0</td>\n",
       "      <td>['tableau', 'r', 'python', 'sql']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>ATC</td>\n",
       "      <td>United States</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Job Title: Entry Level Business Analyst / Prod...</td>\n",
       "      <td>['12 hours ago', 'Full-time', 'Health insurance']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Aeronautical Data Analyst</td>\n",
       "      <td>Garmin International, Inc.</td>\n",
       "      <td>Olathe, KS</td>\n",
       "      <td>via Indeed</td>\n",
       "      <td>Overview:\\n\\nWe are seeking a full-time...\\nAe...</td>\n",
       "      <td>['18 hours ago', 'Full-time']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJBZXJvbmF1dGljYWwgRGF0YSBBbm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sql']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Analyst - Consumer Goods - Contract to Hire</td>\n",
       "      <td>Upwork</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>via Upwork</td>\n",
       "      <td>Enthusiastic Data Analyst for processing sales...</td>\n",
       "      <td>['12 hours ago', '15–25 an hour', 'Work from h...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBDb25zdW...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15–25</td>\n",
       "      <td>an hour</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41600.0</td>\n",
       "      <td>['powerpoint', 'excel', 'power_bi']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Data Analyst | Workforce Management</td>\n",
       "      <td>Krispy Kreme</td>\n",
       "      <td>United States</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Overview of Position\\n\\nThis position will be ...</td>\n",
       "      <td>['7 hours ago', '90K–110K a year', 'Contractor']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgfCBXb3JrZm...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90K–110K</td>\n",
       "      <td>a year</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>110000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>['powerpoint', 'excel', 'outlook', 'word']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index                                             title  \\\n",
       "0           0      0                                      Data Analyst   \n",
       "1           1      1                                      Data Analyst   \n",
       "2           2      2                         Aeronautical Data Analyst   \n",
       "3           3      3  Data Analyst - Consumer Goods - Contract to Hire   \n",
       "4           4      4               Data Analyst | Workforce Management   \n",
       "\n",
       "                 company_name            location           via  \\\n",
       "0                        Meta           Anywhere   via LinkedIn   \n",
       "1                         ATC    United States     via LinkedIn   \n",
       "2  Garmin International, Inc.       Olathe, KS       via Indeed   \n",
       "3                      Upwork           Anywhere     via Upwork   \n",
       "4                Krispy Kreme    United States     via LinkedIn   \n",
       "\n",
       "                                         description  \\\n",
       "0  In the intersection of compliance and analytic...   \n",
       "1  Job Title: Entry Level Business Analyst / Prod...   \n",
       "2  Overview:\\n\\nWe are seeking a full-time...\\nAe...   \n",
       "3  Enthusiastic Data Analyst for processing sales...   \n",
       "4  Overview of Position\\n\\nThis position will be ...   \n",
       "\n",
       "                                          extensions  \\\n",
       "0  ['15 hours ago', '101K–143K a year', 'Work fro...   \n",
       "1  ['12 hours ago', 'Full-time', 'Health insurance']   \n",
       "2                      ['18 hours ago', 'Full-time']   \n",
       "3  ['12 hours ago', '15–25 an hour', 'Work from h...   \n",
       "4   ['7 hours ago', '90K–110K a year', 'Contractor']   \n",
       "\n",
       "                                              job_id  \\\n",
       "0  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...   \n",
       "1  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...   \n",
       "2  eyJqb2JfdGl0bGUiOiJBZXJvbmF1dGljYWwgRGF0YSBBbm...   \n",
       "3  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBDb25zdW...   \n",
       "4  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgfCBXb3JrZm...   \n",
       "\n",
       "                                           thumbnail  ... commute_time  \\\n",
       "0  https://encrypted-tbn0.gstatic.com/images?q=tb...  ...          NaN   \n",
       "1  https://encrypted-tbn0.gstatic.com/images?q=tb...  ...          NaN   \n",
       "2                                                NaN  ...          NaN   \n",
       "3                                                NaN  ...          NaN   \n",
       "4  https://encrypted-tbn0.gstatic.com/images?q=tb...  ...          NaN   \n",
       "\n",
       "  salary_pay salary_rate salary_avg salary_min salary_max salary_hourly  \\\n",
       "0  101K–143K      a year   122000.0   101000.0   143000.0           NaN   \n",
       "1        NaN         NaN        NaN        NaN        NaN           NaN   \n",
       "2        NaN         NaN        NaN        NaN        NaN           NaN   \n",
       "3      15–25     an hour       20.0       15.0       25.0          20.0   \n",
       "4   90K–110K      a year   100000.0    90000.0   110000.0           NaN   \n",
       "\n",
       "   salary_yearly salary_standardized  \\\n",
       "0       122000.0            122000.0   \n",
       "1            NaN                 NaN   \n",
       "2            NaN                 NaN   \n",
       "3            NaN             41600.0   \n",
       "4       100000.0            100000.0   \n",
       "\n",
       "                           description_tokens  \n",
       "0           ['tableau', 'r', 'python', 'sql']  \n",
       "1                                          []  \n",
       "2                                     ['sql']  \n",
       "3         ['powerpoint', 'excel', 'power_bi']  \n",
       "4  ['powerpoint', 'excel', 'outlook', 'word']  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data into DataFrame\n",
    "df = pd.read_csv('/Users/sa21/Desktop/Data_Analyst_Job_Trends/data/gsearch_jobs.csv', nrows= 1000)\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'index', 'title', 'company_name', 'location', 'via',\n",
      "       'description', 'extensions', 'job_id', 'thumbnail', 'posted_at',\n",
      "       'schedule_type', 'work_from_home', 'salary', 'search_term', 'date_time',\n",
      "       'search_location', 'commute_time', 'salary_pay', 'salary_rate',\n",
      "       'salary_avg', 'salary_min', 'salary_max', 'salary_hourly',\n",
      "       'salary_yearly', 'salary_standardized', 'description_tokens'],\n",
      "      dtype='object')\n",
      "All together there are 27 columns\n"
     ]
    }
   ],
   "source": [
    "columns_names = df.columns\n",
    "print(columns_names)\n",
    "print('All together there are', len(df.columns), 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 27 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Unnamed: 0           1000 non-null   int64  \n",
      " 1   index                1000 non-null   int64  \n",
      " 2   title                1000 non-null   object \n",
      " 3   company_name         1000 non-null   object \n",
      " 4   location             1000 non-null   object \n",
      " 5   via                  999 non-null    object \n",
      " 6   description          1000 non-null   object \n",
      " 7   extensions           1000 non-null   object \n",
      " 8   job_id               1000 non-null   object \n",
      " 9   thumbnail            635 non-null    object \n",
      " 10  posted_at            998 non-null    object \n",
      " 11  schedule_type        997 non-null    object \n",
      " 12  work_from_home       332 non-null    object \n",
      " 13  salary               92 non-null     object \n",
      " 14  search_term          1000 non-null   object \n",
      " 15  date_time            1000 non-null   object \n",
      " 16  search_location      1000 non-null   object \n",
      " 17  commute_time         0 non-null      float64\n",
      " 18  salary_pay           92 non-null     object \n",
      " 19  salary_rate          92 non-null     object \n",
      " 20  salary_avg           92 non-null     float64\n",
      " 21  salary_min           90 non-null     float64\n",
      " 22  salary_max           90 non-null     float64\n",
      " 23  salary_hourly        54 non-null     float64\n",
      " 24  salary_yearly        37 non-null     float64\n",
      " 25  salary_standardized  92 non-null     float64\n",
      " 26  description_tokens   1000 non-null   object \n",
      "dtypes: float64(7), int64(2), object(18)\n",
      "memory usage: 211.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numbers of rows\n",
    "rows = len(df)\n",
    "print('And there are', rows, 'rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 of 27 features have null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job titles\n",
    "job_title_counts = df['title'].value_counts()\n",
    "job_title_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words \"Data\" and \"Analyst\" are commonly found in most if not all the job postings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  names\n",
    "company_counts = df['company_name'].value_counts()\n",
    "top_15_company = company_counts.head(15)\n",
    "top_15_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 companies\n",
    "top_15_company = company_counts.head(15)\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_15_company.values, y=top_15_company.index, palette='cividis')\n",
    "plt.title('Top 15 Companies by Job Postings')\n",
    "plt.xlabel('Number of Job Postings')\n",
    "plt.ylabel('Company Name')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows that Upwork is the most active company in job postings. This could mean that freelance or gig-based roles or in high demand. Companies like Talentify.io and Walmart follow but with fewer postings. Other companies posted far less compare to the top 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedules_typ = df['schedule_type'].value_counts(dropna=False)\n",
    "schedules_typ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schedule_type column shows that many job postings list multiple work schedule, such as \"Full-time and Contractor\" or \"Part-time and Temp work.\" Full-time roles dominate the dataset, followed by Contractor and Contractor and Temp work combinations. This suggests that while single schedule listings are common, most postings offer hybrid or flexible options. To better understand the frequency of different work schedules,I'll visualize the top 40 most common combinations in a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count values including NaNs being replaced with 'Missing' for plotting \n",
    "schedule_counts = df['schedule_type'].fillna('Missing').value_counts()\n",
    "\n",
    "# Filter to only show types with 40 or more postings\n",
    "filtered_counts = schedule_counts[schedule_counts >= 40]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=filtered_counts.index, y=filtered_counts.values, palette='viridis')\n",
    "\n",
    "plt.title('Job Schedule Types with 40+ Postings')\n",
    "plt.xlabel('Schedule Type')\n",
    "plt.ylabel('Number of Postings')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schedule type transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "df['schedule_type'] = df['schedule_type'].fillna(\"\")\n",
    "\n",
    "# Standardize and normalize separators\n",
    "df['schedule_type_clean'] = (\n",
    "    df['schedule_type']\n",
    "    .str.lower()\n",
    "    .str.replace(r'\\s*,\\s*', ',', regex=True)\n",
    "    .str.replace(r'\\s+and\\s+', ',', regex=True)\n",
    ")\n",
    "# Split into list and clean individual entries\n",
    "def clean_label_list(lst):\n",
    "    return [x.strip().removeprefix('and ').strip() for x in lst if x.strip()]\n",
    "\n",
    "df['schedule_type_list'] = df['schedule_type_clean'].str.split(',')\n",
    "df['schedule_type_list'] = df['schedule_type_list'].apply(clean_label_list)\n",
    "\n",
    "# renaming to unify variations\n",
    "rename_map = {\n",
    "    'full time': 'full-time',\n",
    "    'fulltime': 'full-time',\n",
    "    'part time': 'part-time',\n",
    "    'temp': 'temp work',\n",
    "    'temporary': 'temp work'\n",
    "}\n",
    "df['schedule_type_list'] = df['schedule_type_list'].apply(\n",
    "    lambda lst: [rename_map.get(x, x) for x in lst]\n",
    ")\n",
    "\n",
    "# One-hot encode schedule types\n",
    "mlb = MultiLabelBinarizer()\n",
    "schedule_dummies = pd.DataFrame(\n",
    "    mlb.fit_transform(df['schedule_type_list']),\n",
    "    columns=[f'is_{x.replace(\"-\", \"_\").replace(\" \", \"_\")}' for x in mlb.classes_],\n",
    "    index=df.index\n",
    ")\n",
    "# Merge into original DataFrame\n",
    "df = pd.concat([df, schedule_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used one-hot encoding to transform the schedule types like full-time or part-time into separate binary columns such as is_full_time. I used MultiLabelBinarizer because some job postings list multiple schedule types in a single row, and this tool helps break them out properly into different variables. I then merged them back into the original DataFrame. The result is a cleaner structure where each schedule type has its own column showing whether it's present in a given job posting. This allows me to get a clearer picture of the schedule types across all postings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each schedule type\n",
    "schedule_columns = [col for col in df.columns if col.startswith('is_')]\n",
    "schedule_sums = df[schedule_columns].sum().sort_values(ascending=False)\n",
    "schedule_labels = schedule_sums.index.str.replace('is_', '').str.replace('_', ' ').str.title()\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=schedule_sums.values, y=schedule_labels, palette='mako')\n",
    "plt.title('Job Postings by Schedule Type (Separated)')\n",
    "plt.xlabel('Number of Postings')\n",
    "plt.ylabel('Schedule Type')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency of job postings by schedule type after cleaning and standardizing the data. Many entries originally included multiple schedule types so I split and normalized them to count each type individually. Full-time roles are by far the most common, followed by contractor and temp work. Part-time, internship, and volunteer roles appear much less frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sets for top 3 types\n",
    "set_full_time = set(df[df.get('is_full_time', 0) == 1].index)\n",
    "set_contractor = set(df[df.get('is_contractor', 0) == 1].index)\n",
    "set_temp = set(df[df.get('is_temp_work', 0) == 1].index)\n",
    "\n",
    "# Plot Venn diagram\n",
    "plt.figure(figsize=(6, 4))\n",
    "venn3([set_full_time, set_contractor, set_temp],\n",
    "      set_labels=('Full-time', 'Contractor', 'Temp Work'))\n",
    "plt.title('Overlap of Top 3 Schedule Types')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the information gathered from the first bar plot, I created a Venn diagram to explore where the top schedule types; Full-time, Contractor, and Temp Work overlap. This visual helps uncover how often these job types are combined in single postings. For example, a number of postings offer both full-time and contractor roles, while some include all three categories. Understanding these overlaps help us identifying hybrid job opportunities and understand how flexible employers are with scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter numerical and categorical columns\n",
    "numer_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "numer_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude unwanted columns \n",
    "exclude_cols = ['Unnamed: 0', 'index', 'commute_time', 'is_', 'is_and_contractor', \n",
    "                'is_and_internship', 'is_and_per_diem', 'is_and_temp_work', \n",
    "                'is_contractor', 'is_full-time', 'is_internship', 'is_part-time', \n",
    "                'is_per_diem', 'is_temp_work', 'is_volunteer','is_full_time', 'is_part_time']\n",
    "\n",
    "# Filter numerical features \n",
    "numer_features = [col for col in numer_features if col not in exclude_cols]\n",
    "# Grid setup\n",
    "n_cols = 2\n",
    "n_rows = (len(numer_features) + 1) // n_cols  # Dynamic row calculation\n",
    "plt.figure(figsize=(18, 12)) \n",
    "\n",
    "# Loop through and plot each numeric feature\n",
    "for i, col in enumerate(numer_features, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.histplot(df[col], bins=20, color='aqua', edgecolor='purple')\n",
    "    plt.title(col, fontsize=14)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the salary features are right-skewed, with values concentrated on the lower end of the distribution and a long tail stretching toward higher salaries. This is especially noticeable in features like salary_avg, salary_min, and salary_max, where a small number of outliers inflate the upper range. While salary_standardized shows a slightly more compressed shape, it still maintains a right-skewed pattern rather than a true bell curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'salary_rate' with 'Missing'\n",
    "df['salary_rate'] = df['salary_rate'].fillna('Missing')\n",
    "\n",
    "# Count the occurrences of each salary rate\n",
    "salary_rate_counts = df['salary_rate'].value_counts().reset_index()\n",
    "salary_rate_counts.columns = ['salary_rate', 'count']\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=salary_rate_counts, x='count', y='salary_rate', palette='viridis')\n",
    "plt.title('Distribution of Salary Rate')\n",
    "plt.xlabel('Number of Postings')\n",
    "plt.ylabel('Salary Rate')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one-hot schedule columns\n",
    "schedule_columns = [col for col in df.columns if col.startswith('is_')]\n",
    "\n",
    "# Compute average salary for each schedule type\n",
    "avg_salary_by_schedule = {\n",
    "    col.replace('is_', '').replace('_', ' ').title(): df.loc[df[col] == 1, 'salary_standardized'].mean()\n",
    "    for col in schedule_columns\n",
    "}\n",
    "# Convert to pandas Series and drop NaN values\n",
    "avg_salary_by_schedule = pd.Series(avg_salary_by_schedule).dropna().sort_values(ascending=False)\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Format x-axis to display currency\n",
    "plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'${int(x):,}'))\n",
    "sns.barplot(x=avg_salary_by_schedule.values, y=avg_salary_by_schedule.index, palette='coolwarm')\n",
    "plt.title('Average Salary by Schedule Type')\n",
    "plt.xlabel('Average Salary')\n",
    "plt.ylabel('Schedule Type')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, full-time employees earn the highest average salaries, followed by contractors and temp workers. Full-time workers make nearly double the average salary of internship positions, highlighting the disparity in compensation based on employment type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and standardize the salary_pay column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean salary pay function\n",
    "def clean_salary_pay(s):\n",
    "    if pd.isna(s) or s == '':\n",
    "        return np.nan\n",
    "\n",
    "    # Convert to string and remove unwanted symbols\n",
    "    s = str(s).replace('$', '').replace(',', '').lower()\n",
    "\n",
    "    # Handle \"K\" notation (e.g., 90K)\n",
    "    s = re.sub(r'(\\d+)k', lambda m: str(float(m.group(1)) * 1000), s)\n",
    "\n",
    "    # Split on common dash types\n",
    "    parts = re.split(r'[\\–\\-]', s)  # en dash or hyphen\n",
    "\n",
    "    try:\n",
    "        # Convert both ends of range to float\n",
    "        nums = [float(p.strip()) for p in parts if p.strip() != '']\n",
    "        if len(nums) == 2:\n",
    "            return sum(nums) / 2  # average\n",
    "        elif len(nums) == 1:\n",
    "            return nums[0]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "# Apply function\n",
    "df['salary_pay_cleaned'] = df['salary_pay'].apply(clean_salary_pay)\n",
    "\n",
    "# fill NaNs with mean\n",
    "df['salary_pay_cleaned'] = df['salary_pay_cleaned'].fillna(df['salary_pay_cleaned'].mean())\n",
    "# Apply function\n",
    "df['salary_pay_cleaned'] = df['salary_pay'].apply(clean_salary_pay)\n",
    "\n",
    "# fill NaNs with mean\n",
    "df['salary_pay_cleaned'] = df['salary_pay_cleaned'].fillna(df['salary_pay_cleaned'].mean())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function helped me standardizes the messy salary_pay column by converting ranges \"$50K–60K\" into averaged numerical values and handles \"K\" notation for thousands. It cleans symbols, splits salary ranges like \"50-60\" into one number by taking the average, and imputes missing values with the mean. The result is an all numeric column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to top 10 most frequent job titles\n",
    "top_titles = df['title'].value_counts().head(10).index\n",
    "df_top = df[df['title'].isin(top_titles)]\n",
    "\n",
    "# Calculate average salary_pay_cleaned by job title - FIXED GROUPBY\n",
    "salary_by_title = df_top.groupby('title')['salary_pay_cleaned'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Plotting - FIXED BARPLOT PARAMETERS\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x=salary_by_title.values, y=salary_by_title.index, palette='coolwarm')\n",
    "\n",
    "# Format x-axis to display currency\n",
    "plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'${int(x):,}'))\n",
    "plt.title('Average Salary Pay by Top 10 Job Titles', pad=20)\n",
    "plt.xlabel('Average Salary Pay', labelpad=10)\n",
    "plt.ylabel('Job Title', labelpad=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average salary pay across the top 10 job titles shows a narrow range. And roles like Data Analyst II, the pay is even closer. Data Engineer tops the list, but even the lowest ranked position Business Data Analyst is not far behind. This suggests a competitive and balanced compensation landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot of cleaned salary rate types\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=df, x='salary_rate', order=df['salary_rate'].value_counts().index, palette='Set2')\n",
    "plt.title('Distribution of Salary Rate Types')\n",
    "plt.xlabel('Salary Rate')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace any NaNs in work_from_home with False or a neutral label\n",
    "df['work_from_home'] = df['work_from_home'].fillna(False)\n",
    "\n",
    "# Group and plot\n",
    "avg_salary_remote = df.groupby('work_from_home')['salary_pay_cleaned'].mean().dropna()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=avg_salary_remote.index.astype(str), y=avg_salary_remote.values, palette='pastel')\n",
    "# Format x-axis to display currency\n",
    "plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'${int(x):,}'))\n",
    "plt.title('Average Salary by Work from Home Status')\n",
    "plt.xlabel('Remote Job (True/False)')\n",
    "plt.ylabel('Average Salary')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most job postings have a blank value for the work-from-home column, so I chose to fill the missing rows with False, assuming they represent non-remote positions. I then compared the average salary between remote (True) and non-remote (False) jobs. The results showed that the average salary for non-remote jobs is significantly higher. If this assumption is true, it would suggest that remote jobs tend to pay less on average than in-person roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert salary_rate to lowercase and strip whitespace\n",
    "df['salary_rate_cleaned'] = df['salary_rate'].str.lower().str.strip()\n",
    "\n",
    "# Convert salary_min and salary_max to numeric\n",
    "df['salary_min'] = pd.to_numeric(df['salary_min'], errors='coerce')\n",
    "df['salary_max'] = pd.to_numeric(df['salary_max'], errors='coerce')\n",
    "\n",
    "# Convert hourly salaries to yearly\n",
    "hourly_mask = df['salary_rate_cleaned'] == 'an hour'\n",
    "df.loc[hourly_mask, 'salary_min'] = df.loc[hourly_mask, 'salary_min'] * 40 * 52\n",
    "df.loc[hourly_mask, 'salary_max'] = df.loc[hourly_mask, 'salary_max'] * 40 * 52\n",
    "\n",
    "# Fill NaNs with average values (optional)\n",
    "df['salary_min'] = df['salary_min'].fillna(df['salary_min'].mean())\n",
    "df['salary_max'] = df['salary_max'].fillna(df['salary_max'].mean())\n",
    "\n",
    "# Limit to top job titles\n",
    "top_titles = df['title'].value_counts().head(10).index\n",
    "df_top = df[df['title'].isin(top_titles)]\n",
    "\n",
    "# Calculate average salary_min and salary_max for each title\n",
    "salary_range = df_top.groupby('title')[['salary_min', 'salary_max']].mean().sort_values(by='salary_max', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 7))\n",
    "for i, (index, row) in enumerate(salary_range.iterrows()):\n",
    "    plt.plot([row['salary_min'], row['salary_max']], [index, index], marker='|', color='teal', linewidth=4)\n",
    "\n",
    "plt.title('Average Salary Range by Job Title')\n",
    "plt.xlabel('Salary ($)')\n",
    "plt.ylabel('Job Title')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The salary ranges across different data-related job titles appear relatively consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job Description NLP Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "df['description_cleaned'] = df['description'].apply(preprocess_text)\n",
    "df['description_cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_salary(text):\n",
    "    match = re.search(r'\\$?\\d{2,3}(?:,\\d{3})?(?: ?- ?\\$?\\d{2,3}(?:,\\d{3})?)?', text)\n",
    "    return match.group() if match else None\n",
    "\n",
    "df['extracted_salary'] = df['description_cleaned'].apply(extract_salary)\n",
    "df['extracted_salary'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_remote_status(text):\n",
    "    if 'remote' in text or 'work from home' in text:\n",
    "        return 'Remote'\n",
    "    elif 'hybrid' in text:\n",
    "        return 'Hybrid'\n",
    "    elif any(kw in text for kw in [\n",
    "        'on-site', 'on site', 'in-office', 'in office', 'in-person', 'in person',\n",
    "        'must be on site', 'work from office', 'report to office',\n",
    "        'office based', 'office setting', 'onsite presence required',\n",
    "        'this is not a remote position', 'required to be physically present',\n",
    "        'must be available on location'\n",
    "    ]):\n",
    "        return 'On-site'\n",
    "    return None\n",
    "\n",
    "\n",
    "df['remote_status_extracted'] = df['description_cleaned'].apply(extract_remote_status)\n",
    "df['remote_status_extracted'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_degree(text):\n",
    "    if 'phd' in text or 'doctoral' in text:\n",
    "        return \"PhD\"\n",
    "    elif 'master' in text or 'graduate degree' in text:\n",
    "        return \"Master's\"\n",
    "    elif 'bachelor' in text:\n",
    "        return \"Bachelor's\"\n",
    "    elif 'associate' in text:\n",
    "        return \"Associate\"\n",
    "    elif 'high school' in text or 'no degree' in text:\n",
    "        return \"None\"\n",
    "    return None\n",
    "\n",
    "df['degree_required_extracted'] = df['description_cleaned'].apply(extract_degree)\n",
    "df['degree_required_extracted'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_experience(text):\n",
    "    if 'entry level' in text:\n",
    "        return 'Entry-level'\n",
    "    elif re.search(r'\\b\\d{1,2}[- ]?(plus|\\+)? (years|yrs) (of )?(experience|exp)', text):\n",
    "        return 'Experienced'\n",
    "    elif 'senior' in text or 'lead role' in text:\n",
    "        return 'Senior-level'\n",
    "    return None\n",
    "\n",
    "df['experience_level_extracted'] = df['description_cleaned'].apply(extract_experience)\n",
    "df['experience_level_extracted'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_benefits(text):\n",
    "    benefits = []\n",
    "    if 'health insurance' in text:\n",
    "        benefits.append('Health Insurance')\n",
    "    if '401k' in text:\n",
    "        benefits.append('401k')\n",
    "    if 'bonus' in text:\n",
    "        benefits.append('Bonus')\n",
    "    if 'pto' in text or 'paid time off' in text:\n",
    "        benefits.append('PTO')\n",
    "    if 'stock' in text or 'equity' in text:\n",
    "        benefits.append('Equity')\n",
    "    return ', '.join(benefits) if benefits else None\n",
    "\n",
    "df['benefits'] = df['description_cleaned'].apply(extract_benefits)\n",
    "df['benefits'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_visa_info(text):\n",
    "    if 'visa sponsorship' in text or 'sponsorship available' in text:\n",
    "        return 'Sponsorship Available'\n",
    "    elif 'must be authorized to work' in text:\n",
    "        return 'No Sponsorship'\n",
    "    return None\n",
    "\n",
    "df['visa_sponsorship'] = df['description_cleaned'].apply(extract_visa_info)\n",
    "df['visa_sponsorship'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_schedule(text):\n",
    "    if 'full-time' in text or 'full time' in text:\n",
    "        return 'Full-time'\n",
    "    elif 'part-time' in text or 'part time' in text:\n",
    "        return 'Part-time'\n",
    "    elif 'contract' in text:\n",
    "        return 'Contract'\n",
    "    elif 'internship' in text:\n",
    "        return 'Internship'\n",
    "    return None\n",
    "\n",
    "df['schedule_type_extracted'] = df['description_cleaned'].apply(extract_schedule)\n",
    "df['schedule_type_extracted'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "df['travel_required'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_list = [  'python', 'r', 'sql', 'excel', 'tableau', 'power bi', 'alteryx', 'looker', \n",
    "    'sas', 'spark', 'hadoop', 'aws', 'azure', 'gcp', 'bigquery', 'snowflake',\n",
    "    'matplotlib', 'seaborn', 'plotly', 'dash',\n",
    "    'scikit-learn', 'tensorflow', 'keras', 'pytorch',\n",
    "    'numpy', 'pandas', 'statsmodels',\n",
    "    'git', 'jira', 'bash', 'linux',\n",
    "    'apache airflow', 'airflow',\n",
    "    'etl', 'bi tools', 'data warehouse', 'data lake',\n",
    "    'machine learning', 'deep learning', 'statistics', 'predictive modeling',\n",
    "    'regression', 'clustering', 'nlp', 'natural language processing',\n",
    "    'a/b testing', 'experimentation', 'data visualization']\n",
    "\n",
    "def extract_skills(text):\n",
    "    found = [skill for skill in skills_list if skill in text]\n",
    "    return ', '.join(found) if found else None\n",
    "\n",
    "df['skills_mentioned'] = df['description_cleaned'].apply(extract_skills)\n",
    "df['skills_mentioned'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# List of U.S. states and abbreviations\n",
    "us_states = [\n",
    "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\",\n",
    "    \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n",
    "    \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\",\n",
    "    \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\",\n",
    "    \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\",\n",
    "    \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\",\n",
    "    \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\",\n",
    "    \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
    "]\n",
    "\n",
    "state_abbr = [\n",
    "    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\",\n",
    "    \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\",\n",
    "    \"NH\", \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\",\n",
    "    \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\", \"DC\"\n",
    "]\n",
    "\n",
    "# Combine all valid names/abbreviations\n",
    "state_set = set(state_abbr + us_states)\n",
    "\n",
    "def extract_state(text):\n",
    "    if pd.isnull(text):\n",
    "        return None\n",
    "    words = re.findall(r'\\b[A-Za-z]{2,}\\b', text)\n",
    "    for word in words:\n",
    "        if word.upper() in state_set or word.title() in state_set:\n",
    "            return word.upper() if word.upper() in state_abbr else word.title()\n",
    "    return None\n",
    "\n",
    "df['extracted_state'] = df['description_cleaned'].apply(extract_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all rows in output\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Now show all state counts\n",
    "print(df['extracted_state'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your CSV\n",
    "df1 = pd.read_csv('/Users/sa21/Desktop/Data_Analyst_Job_Trends/data/cleaned_gsearch_jobs.csv')\n",
    "\n",
    "# View all missing data counts\n",
    "missing_data = df1.isnull().sum()\n",
    "print(missing_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_names2 = df1.columns\n",
    "columns_names2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['work_from_home'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the cleaned CSV file \n",
    "df1 = pd.read_csv(\"/Users/sa21/Desktop/Data_Analyst_Job_Trends/data/cleaned_gsearch_jobs.csv\")\n",
    "\n",
    "# Save as Excel to a subfolder\n",
    "df1.to_excel('/Users/sa21/Desktop/Data_Analyst_Job_Trends/data/processed/cleaned_gsearch_jobs.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "\n",
    "def standardize_column_text(df, columns_to_clean):\n",
    "    \"\"\"\n",
    "    Cleans and standardizes text fields in a DataFrame using regex rules per column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns_to_clean (list): List of column names to standardize.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with cleaned columns.\n",
    "    \"\"\"\n",
    "    df1 = df1.copy()\n",
    "\n",
    "    for col in columns_to_clean:\n",
    "        def clean_text(text):\n",
    "            if pd.isnull(text):\n",
    "                return text\n",
    "            text = str(text).lower()\n",
    "\n",
    "            # Title column\n",
    "            if col == 'title':\n",
    "                text = re.sub(r'(sr\\.?|senior)', 'senior', text)\n",
    "                text = re.sub(r'(jr\\.?|junior)', 'junior', text)\n",
    "                text = re.sub(r'\\s*-\\s*.*$', '', text)\n",
    "                text = re.sub(r'[^\\w\\s]', '', text)\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "            # Company name\n",
    "            elif col == 'company_name':\n",
    "                text = re.sub(r'[^\\w\\s]', '', text)\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "            # Location\n",
    "            elif col == 'location':\n",
    "                text = re.sub(r'\\b(remote|anywhere|united states)\\b', '', text)\n",
    "                text = re.sub(r'[^\\w\\s,]', '', text)\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "            # Via\n",
    "            elif col == 'via':\n",
    "                text = re.sub(r'[^\\w\\s]', '', text)\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "            # Description\n",
    "            elif col == 'description':\n",
    "                text = re.sub(r'<.*?>', '', text)\n",
    "                text = re.sub(r\"(?:bachelor(?:'s)?|ba|bs|bsc)\", 'bachelor', text)\n",
    "                text = re.sub(r\"(?:master(?:'s)?|ms|msc|mba)\", 'master', text)\n",
    "                text = re.sub(r\"(?:phd|doctorate)\", 'phd', text)\n",
    "                text = re.sub(r'degree in [a-z\\s]+', 'degree', text)\n",
    "                text = re.sub(r'[^\\w\\s]', '', text)\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "            # Extensions\n",
    "            elif col == 'extensions':\n",
    "                text = re.sub(r'[\\[\\]\\']', '', text)\n",
    "                text = re.sub(r'[^\\w\\s,]', '', text)\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "            # Schedule type\n",
    "            elif col == 'schedule_type':\n",
    "                text = re.sub(r'[^a-z\\s]', '', text)\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "            # Salary rate\n",
    "            elif col == 'salary_rate':\n",
    "                text = re.sub(r'\\b(an?|per)\\b', '', text)\n",
    "                text = re.sub(r'[^\\w\\s]', '', text)\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "            # Skills found (stringified list)\n",
    "            elif col == 'skills_found':\n",
    "                try:\n",
    "                    skills = ast.literal_eval(text)\n",
    "                    if isinstance(skills, list):\n",
    "                        cleaned_skills = [re.sub(r'[^\\w\\s]', '', s.strip().lower()) for s in skills if isinstance(s, str)]\n",
    "                        text = ', '.join(sorted(set(cleaned_skills)))\n",
    "                    else:\n",
    "                        text = ''\n",
    "                except:\n",
    "                    text = ''\n",
    "\n",
    "            else:\n",
    "                # Fallback for any general text column\n",
    "                text = re.sub(r'[^\\w\\s]', '', text)\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "            return text\n",
    "\n",
    "        df1[col] = df1[col].astype(str).apply(clean_text)\n",
    "\n",
    "    return df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
